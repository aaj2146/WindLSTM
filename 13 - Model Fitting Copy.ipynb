{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from datetime import date, datetime, timedelta\n",
    "import pickle\n",
    "import plotly.offline as py\n",
    "import plotly.graph_objs as go\n",
    "from plotly import __version__\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "init_notebook_mode(connected=True)\n",
    "import cufflinks as cf\n",
    "import seaborn as sns\n",
    "from scipy import signal, stats\n",
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "turbine_readings = pd.read_csv(\"./data/WindData_scrubbed.csv\")\n",
    "weather_data = pd.read_csv(\"hourly_weather_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "turbine_readings = turbine_readings.iloc[:, 0:74]\n",
    "mean_readings = turbine_readings.apply(lambda x: np.mean(x), axis=1)\n",
    "median_readings = turbine_readings.apply(lambda x: np.median(x), axis=1)\n",
    "print(turbine_readings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data = weather_data.dropna(axis=1, how=\"all\")\n",
    "weather_data.fillna(method=\"ffill\", inplace=True)\n",
    "weather_data[\"timestamp\"] = pd.to_datetime(weather_data[\"valid_time_gmt\"], unit=\"s\") - pd.Timedelta(\"08:00:00\")\n",
    "weather_data[\"weather_date\"] = weather_data[\"timestamp\"].dt.date\n",
    "weather_data[\"weather_time\"] = weather_data[\"timestamp\"].dt.time\n",
    "weather_data[\"weather_month\"] = weather_data[\"timestamp\"].dt.month\n",
    "weather_data[\"weather_day_of_month\"] = weather_data[\"timestamp\"].dt.day\n",
    "weather_data[\"weather_day_of_week\"] = weather_data[\"timestamp\"].dt.dayofweek\n",
    "weather_data.set_index(\"timestamp\", inplace=True)\n",
    "windspeed = weather_data[\"wspd\"]\n",
    "windspeed_cubed = windspeed**3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cross_correlation(feature, readings, interval):\n",
    "    feature = feature.resample(interval).asfreq()\n",
    "    feature.interpolate(method=\"linear\", inplace=True)\n",
    "    # signal.correlate calculates the integral(area) of the product of shifting time series\n",
    "    cross_corrs_valid = signal.correlate(feature, readings, mode=\"valid\", method=\"direct\")\n",
    "    print(\"Valid Shape: \", cross_corrs_valid.shape)\n",
    "    cross_corrs_full = signal.correlate(feature, readings, mode=\"full\", method=\"direct\")\n",
    "    print(\"Full Shape: \", cross_corrs_full.shape)\n",
    "    #print(len(feature.index), cross_corrs_valid.shape)\n",
    "    trace = go.Scatter(\n",
    "        x = feature.index,\n",
    "        y = cross_corrs_valid\n",
    "    )\n",
    "    data = [trace]\n",
    "    py.iplot(data)\n",
    "    plt.figure(figsize=(15,8))\n",
    "    plt.plot(cross_corrs_full)\n",
    "    plt.axvline(x=readings.shape[0]-1, color=\"red\")\n",
    "    plt.axvline(x=cross_corrs_full.shape[0]-readings.shape[0], color=\"red\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pearson_correlation(feature, readings, interval):\n",
    "    feature = feature.resample(interval).asfreq()\n",
    "    feature.interpolate(method=\"linear\", inplace=True)\n",
    "    # signal.correlate calculates the integral(area) of the product of shifting time series\n",
    "    pears_corrs = pearsonr(feature, readings)\n",
    "    print(\"pearsonr Shape: \", pears_corrs.shape)\n",
    "    cross_corrs_full = signal.correlate(feature, readings, mode=\"full\", method=\"direct\")\n",
    "    print(\"Full Shape: \", cross_corrs_full.shape)\n",
    "    #print(len(feature.index), cross_corrs_valid.shape)\n",
    "    trace = go.Scatter(\n",
    "        x = feature.index,\n",
    "        y = pears_corrs[0]\n",
    "    )\n",
    "    data = [trace]\n",
    "    py.iplot(data)\n",
    "    plt.figure(figsize=(15,8))\n",
    "    plt.plot(cross_corrs_full)\n",
    "    plt.axvline(x=readings.shape[0]-1, color=\"red\")\n",
    "    plt.axvline(x=cross_corrs_full.shape[0]-readings.shape[0], color=\"red\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cross_correlation(windspeed_cubed, mean_readings, \"1Min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cross_correlation(windspeed_cubed, mean_readings, \"5Min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cross_correlation(windspeed_cubed, mean_readings, \"20Min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cross_correlation(windspeed_cubed, mean_readings, \"1H\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cross_correlation(windspeed_cubed, mean_readings, \"100Min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stats.pearsonr(turbine_readings.iloc[:, 0], turbine_readings.iloc[:, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.corrcoef(turbine_readings.iloc[:, 0], turbine_readings.iloc[:, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "windspeed_5min = windspeed_cubed.resample(\"5Min\").asfreq()\n",
    "windspeed_5min.interpolate(method=\"linear\", inplace=True)\n",
    "correlations = []\n",
    "for i in range(0, (len(windspeed_5min)-len(mean_readings)+1)):\n",
    "    correlations.append(stats.pearsonr(mean_readings, windspeed_5min.iloc[i:i+mean_readings.shape[0]])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(correlations))\n",
    "trace = go.Scatter(\n",
    "    x = windspeed_5min.index,\n",
    "    y = correlations\n",
    ")\n",
    "data = [trace]\n",
    "py.iplot(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling the GE output (Regression)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = windspeed_cubed\n",
    "feature = feature.resample('5Min').asfreq()\n",
    "feature.interpolate(method=\"linear\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_array = signal.correlate(feature, mean_readings, mode='valid', method = 'direct')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peak_timestamp = feature.index[np.argmax(corr_array)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peak_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranga = pd.date_range(start= peak_timestamp, periods = 2600, freq='5T')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranga"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_turbine = pd.DataFrame(list(mean_readings), index=ranga)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ge_data = time_turbine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ge_data['wspd'] = feature[time_turbine.index]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_temp = weather_data.temp\n",
    "feature_temp = feature_temp.resample('5Min').asfreq()\n",
    "feature_temp.interpolate(method=\"linear\", inplace=True)\n",
    "ge_data['temp'] = feature_temp[time_turbine.index]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_wdir = weather_data.wdir\n",
    "feature_wdir = feature_wdir.resample('5Min').asfreq()\n",
    "feature_wdir.interpolate(method=\"linear\", inplace=True)\n",
    "ge_data['wdir'] = feature_wdir[time_turbine.index]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ge_data.rename(columns = {0:'output'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ge_data['dt_col'] = pd.to_datetime(ge_data.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ge_data = ge_data.set_index('dt_col')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ge_output = ge_data[['output']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ge_output = pd.Series(ge_data.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.offline as py\n",
    "import plotly.graph_objs as go\n",
    "from plotly import __version__\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot, plot_mpl\n",
    "init_notebook_mode(connected=True)\n",
    "#from plotly.offline import plot_mpl\n",
    "\n",
    "#from plotly.plotly import plot_mpl\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "result = seasonal_decompose(ge_output, freq = 288)\n",
    "fig = result.plot()\n",
    "plot_mpl(fig)\n",
    "\n",
    "#fig = go.Figure(d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.vector_ar.vecm import coint_johansen\n",
    "#since the test works for only 12 variables, I have randomly dropped\n",
    "#in the next iteration, I would drop another and check the eigenvalues\n",
    "#johan_test_temp = data.drop([ 'CO(GT)'], axis=1)\n",
    "coint_johansen(ge_data,-1,1).eig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coint_johansen(ge_data[[\"output\",\"wspd\"]],-1,1).eig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "minmax = MinMaxScaler(feature_range=(0,1))\n",
    "\n",
    "\n",
    "ge_data.wspd = scaler.fit_transform(np.array(ge_data.wspd.values).reshape(-1,1)) \n",
    "ge_data.wdir = scaler.fit_transform(np.array(ge_data.wdir.values).reshape(-1,1))\n",
    "ge_data.temp = scaler.fit_transform(np.array(ge_data.temp.values).reshape(-1,1)) \n",
    "\n",
    "\n",
    "ge_data.output = minmax.fit_transform(np.array(ge_data.output.values).reshape(-1,1))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the train and validation set\n",
    "train = ge_data[:int(0.8*(len(ge_data)))]\n",
    "valid = ge_data[int(0.8*(len(ge_data))):]\n",
    "\n",
    "#fit the model\n",
    "from statsmodels.tsa.vector_ar.var_model import VAR\n",
    "\n",
    "model1 = VAR(endog=train)\n",
    "model_fit = model1.fit()\n",
    "\n",
    "# make prediction on validation\n",
    "prediction = model_fit.forecast(model_fit.y, steps=len(valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline Model Random Forest\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "param_grid = {'bootstrap': [True, False],\n",
    " 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None],\n",
    " 'max_features': ['auto', 'sqrt'],\n",
    " 'min_samples_leaf': [1, 2, 4],\n",
    " 'min_samples_split': [2, 5, 10],\n",
    " 'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]}\n",
    "\n",
    "\n",
    "rf = RandomForestRegressor()\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = param_grid, n_iter = 100, cv = 4, verbose=2, random_state=42, n_jobs = -1)\n",
    "# Fit the random search model\n",
    "rf_random.fit(train[['wspd','temp','wdir']], train.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_random.score(valid[['wspd','temp','wdir']], valid.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The RMSE for baseline RF model is: \" ,np.sqrt(mean_squared_error( y_pred=rf_random.predict(valid[['wspd','temp','wdir']]), y_true=valid.output)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = train[[\"wspd\",\"temp\",\"wdir\"]]\n",
    "train_y = train.output\n",
    "\n",
    "val_X = valid[[\"wspd\",\"temp\",\"wdir\"]]\n",
    "val_y = valid.output\n",
    "\n",
    "train_X = train_X.values\n",
    "val_X = val_X.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))\n",
    "#test_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))\n",
    "val_X = val_X.reshape((val_X.shape[0], 1, val_X.shape[1]))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM, GRU\n",
    "\n",
    "\n",
    "# design network\n",
    "model = Sequential()\n",
    "#model.add(LSTM())\n",
    "\n",
    "model.add(GRU(24, input_shape=(train_X.shape[1], train_X.shape[2])))\n",
    "#model.add(GRU(100))\n",
    "\n",
    "\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "# fit network\n",
    "history = model.fit(train_X, train_y, epochs=75, batch_size=300, validation_data=(val_X, val_y), verbose=2, shuffle=False)\n",
    "# plot history\n",
    "#import matplotlib.pyplot as pyplot\n",
    "#pyplot.plot(history.history['loss'], label='train')\n",
    "#pyplot.plot(history.history['val_loss'], label='test')\n",
    "#pyplot.legend()\n",
    "#pyplot.show()\n",
    "\n",
    "\n",
    "#y_pred = model.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(val_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ge_data.columns\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "#converting predictions to dataframe\n",
    "pred = pd.DataFrame(index=range(0,len(prediction)),columns=[cols])\n",
    "for j in range(0,3):\n",
    "    for i in range(0, len(prediction)):\n",
    "        pred.iloc[i][j] = prediction[i][j]\n",
    "\n",
    "#check rmse\n",
    "for i in cols:\n",
    "    print('rmse value for', i, 'is : ', np.sqrt(mean_squared_error(pred[i], valid[i])))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.output = pred.output.astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.output = np.array(pred.output).reshape(-1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.offline as py\n",
    "import plotly.graph_objs as go\n",
    "from plotly import __version__\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "\n",
    "#traces = []\n",
    "#random_turbines = np.random.choice(turbine_names, size=3, replace=False)\n",
    "\n",
    "\n",
    "trace1 = go.Scatter(x = valid.index,\n",
    "                   y = list(100*val_y),\n",
    "                   mode = 'lines',\n",
    "                   name = 'test')\n",
    "\n",
    "\n",
    "trace2 = go.Scatter(x = valid.index,\n",
    "                   y = np.array(100*y_pred).reshape(-1,),\n",
    "                   mode = 'lines',\n",
    "                   name = 'pred')\n",
    "\n",
    "\n",
    "\n",
    "traces=[trace1, trace2]\n",
    "\n",
    "\n",
    "\n",
    "layout = go.Layout(\n",
    "    title='GE data LSTM model'\n",
    "    \n",
    ")\n",
    "fig = go.Figure(data=traces, layout=layout)\n",
    "#plot_url = py.plot(fig, filename='multiple-axes-double')\n",
    "\n",
    "py.iplot(fig, filename='line-mode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(mean_squared_error(y_pred, val_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.offline as py\n",
    "import plotly.graph_objs as go\n",
    "from plotly import __version__\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "\n",
    "#traces = []\n",
    "#random_turbines = np.random.choice(turbine_names, size=3, replace=False)\n",
    "\n",
    "\n",
    "trace1 = go.Scatter(x = valid.index,\n",
    "                   y = list(valid.output),\n",
    "                   mode = 'lines',\n",
    "                   name = 'test')\n",
    "\n",
    "\n",
    "trace2 = go.Scatter(x = valid.index,\n",
    "                   y = np.array(pred.output).reshape(-1,),\n",
    "                   mode = 'lines',\n",
    "                   name = 'pred')\n",
    "\n",
    "\n",
    "\n",
    "traces=[trace1, trace2]\n",
    "\n",
    "\n",
    "\n",
    "layout = go.Layout(\n",
    "    title='VAR model'\n",
    "    \n",
    ")\n",
    "fig = go.Figure(data=traces, layout=layout)\n",
    "#plot_url = py.plot(fig, filename='multiple-axes-double')\n",
    "\n",
    "py.iplot(fig, filename='line-mode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.offline as py\n",
    "import plotly.graph_objs as go\n",
    "from plotly import __version__\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "\n",
    "#traces = []\n",
    "#random_turbines = np.random.choice(turbine_names, size=3, replace=False)\n",
    "\n",
    "\n",
    "trace1 = go.Scatter(x = valid.index,\n",
    "                   y = list(100*valid.output),\n",
    "                   mode = 'lines',\n",
    "                   name = 'Actual Output')\n",
    "\n",
    "\n",
    "trace2 = go.Scatter(x = valid.index,\n",
    "                   y = 100*rf_random.predict(valid[['wspd','temp','wdir']]),\n",
    "                   mode = 'lines',\n",
    "                   name = 'Predicted Output')\n",
    "\n",
    "\n",
    "\n",
    "traces=[trace1, trace2]\n",
    "\n",
    "\n",
    "\n",
    "layout = go.Layout(\n",
    "    title='Random Forest model'\n",
    "    \n",
    ")\n",
    "fig = go.Figure(data=traces, layout=layout)\n",
    "#plot_url = py.plot(fig, filename='multiple-axes-double')\n",
    "\n",
    "py.iplot(fig, filename='line-mode')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sotavento Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#soluto_data = pd.read_csv('soluto_windfarm.csv', encoding='latin1')\n",
    "#daily_soluto = pd.read_excel('soluto_daily.xlsx', sheet_name='daily')\n",
    "hourly_soluto = pd.read_excel('soluto_daily.xlsx', sheet_name='hourly')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#soluto_data.Date = pd.to_datetime(soluto_data.Date)\n",
    "hourly_soluto.Date = pd.to_datetime(hourly_soluto.Date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_soluto = hourly_soluto.sort_values(\"Date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_soluto=hourly_soluto.set_index('Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_soluto.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_soluto = hourly_soluto[~hourly_soluto.index.duplicated(keep='first')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting from european format to regular float\n",
    "\n",
    "hourly_soluto['Energy'] = hourly_soluto['Energy'].apply(lambda x: x.replace('.','').replace(',', '.'))\n",
    "hourly_soluto['Speed'] = hourly_soluto['Speed'].apply(lambda x: x.replace('.','').replace(',', '.'))\n",
    "#hourly_soluto['Direction'] = hourly_soluto['Direction'].apply(lambda x: x.replace('.','').replace(\",\",\".\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_soluto1 = hourly_soluto.replace({'-': 0.000001})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_soluto = hourly_soluto.replace({'-': np.nan})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_soluto_p = hourly_soluto.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_soluto_p.Energy = hourly_soluto_p.Energy.astype('float64')\n",
    "hourly_soluto_p.Speed = hourly_soluto_p.Speed.astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = hourly_soluto['2013-01-01 00:00:00' : '2015-12-31 23:00:00']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset.Energy = subset.Energy.astype('float64')\n",
    "subset.Speed = subset.Speed.astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.offline as py\n",
    "import plotly.graph_objs as go\n",
    "from plotly import __version__\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot, plot_mpl\n",
    "init_notebook_mode(connected=True)\n",
    "#from plotly.offline import plot_mpl\n",
    "\n",
    "#from plotly.plotly import plot_mpl\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "result = seasonal_decompose(subset.Speed.values, model='additive', freq = 24*365)\n",
    "fig = result.plot()\n",
    "plot_mpl(fig)\n",
    "\n",
    "#fig = go.Figure(data=traces, layout=layout)\n",
    "#plot_url = py.plot(fig, filename='multiple-axes-double')\n",
    "\n",
    "#py.iplot(fig, filename='line-mode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.offline as py\n",
    "import plotly.graph_objs as go\n",
    "from plotly import __version__\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "\n",
    "data = [go.Scatter(x=hourly_soluto.index, y=hourly_soluto.Energy )]\n",
    "\n",
    "py.iplot(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.offline as py\n",
    "import plotly.graph_objs as go\n",
    "from plotly import __version__\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "\n",
    "traces = []\n",
    "#random_turbines = np.random.choice(turbine_names, size=3, replace=False)\n",
    "\n",
    "\n",
    "for feature in soluto_data.columns[1:]:\n",
    "    #filtered_turbine = daily_mean_turbine_reading.loc[daily_mean_turbine_reading[\"turbine_num\"] == turbine]\n",
    "    trace = go.Scatter(x = soluto_data.Date,\n",
    "                       y = float(soluto_data[feature])/max(float(soluto_data[feature])),\n",
    "                       mode = 'lines',\n",
    "                       name = feature)\n",
    "    \n",
    "traces.append(trace)\n",
    "\n",
    "\n",
    "\n",
    "layout = go.Layout(\n",
    "    title='Time Series - Wind Speed (NOAA data), Turbine Output',\n",
    "    yaxis=dict(\n",
    "        title='Turbine Output (kWh)',\n",
    "        #title='Wind Speed (mph)',\n",
    "        titlefont=dict(\n",
    "            color='blue'\n",
    "        ),\n",
    "        tickfont=dict(\n",
    "            color='blue'\n",
    "        ),\n",
    "    ),\n",
    "    yaxis2=dict(\n",
    "        title='Wind Speed (mph)',\n",
    "        titlefont=dict(\n",
    "            color='orange'\n",
    "        ),\n",
    "        tickfont=dict(\n",
    "            color='orange'\n",
    "        ),\n",
    "        overlaying='y',\n",
    "        side='right'\n",
    "    )\n",
    ")\n",
    "fig = go.Figure(data=traces, layout=layout)\n",
    "#plot_url = py.plot(fig, filename='multiple-axes-double')\n",
    "\n",
    "py.iplot(fig, filename='line-mode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "print(sm.tsa.stattools.grangercausalitytests(subset[['Energy','Speed']],maxlag=1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sm.tsa.stattools.grangercausalitytests(subset[['Speed','Energy']],1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True, feat_names = None):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [(feat_names[j] + '(t-%d)' % i) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [(feat_names[j] + '(t)' ) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = dataset.values\n",
    "# integer encode direction\n",
    "encoder = LabelEncoder()\n",
    "values[:,4] = encoder.fit_transform(values[:,4])\n",
    "# ensure all data is float\n",
    "values = values.astype('float32')\n",
    "# normalize features\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled = scaler.fit_transform(values)\n",
    "# frame as supervised learning\n",
    "reframed = series_to_supervised(scaled, 1, 1)\n",
    "# drop columns we don't want to predict\n",
    "reframed.drop(reframed.columns[[9,10,11,12,13,14,15]], axis=1, inplace=True)\n",
    "print(reframed.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = hourly_soluto_p.values\n",
    "scaler = StandardScaler()\n",
    "scaled = scaler.fit_transform(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reframed = series_to_supervised(scaled, n_in= 24,n_out= 1, feat_names= hourly_soluto_p.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train and test sets\n",
    "values = reframed.values\n",
    "n_train_hours = 365 * 24\n",
    "train = values[:n_train_hours, :]\n",
    "test = values[n_train_hours:, :]\n",
    "# split into input and outputs\n",
    "train_X, train_y = train[:, :-1], train[:, -1]\n",
    "test_X, test_y = test[:, :-1], test[:, -1]\n",
    "# reshape input to be 3D [samples, timesteps, features]\n",
    "train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))\n",
    "test_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))\n",
    "print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = hourly_soluto_p[['Speed', 'Direction']]\n",
    "y = hourly_soluto_p[['Energy']]\n",
    "X_train = X['2006-01-01 01:00:00' : '2016-12-30 23:00:00']\n",
    "X_val = X['2017-01-01 01:00:00' : '2017-12-31 23:00:00']\n",
    "X_test = X['2018-01-01 01:00:00' : '2018-11-30 23:00:00']\n",
    "\n",
    "\n",
    "y_train = y['2006-01-01 01:00:00' : '2016-12-30 23:00:00']\n",
    "y_val = y['2017-01-01 01:00:00' : '2017-12-31 23:00:00']\n",
    "y_test = y['2018-01-01 01:00:00' : '2018-11-30 23:00:00']\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = X_train.values\n",
    "train_y = y_train.values\n",
    "\n",
    "test_X = X_val.values\n",
    "test_y = y_val.values\n",
    "\n",
    "\n",
    "\n",
    "minmax = MinMaxScaler(feature_range=(0,1))\n",
    "\n",
    "train_X = scaler.fit_transform(train_X)\n",
    "train_y = minmax.fit_transform(train_y)\n",
    "\n",
    "test_X = scaler.fit_transform(test_X) \n",
    "test_y = minmax.fit_transform(test_y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))\n",
    "test_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM, GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# design network\n",
    "model = Sequential()\n",
    "#model.add(LSTM())\n",
    "\n",
    "model.add(GRU(75, input_shape=(train_X.shape[1], train_X.shape[2])))\n",
    "\n",
    "\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mae', optimizer='adam')\n",
    "# fit network\n",
    "history = model.fit(train_X, train_y, epochs=100, batch_size=300, validation_data=(test_X, test_y), verbose=2, shuffle=False)\n",
    "# plot history\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='test')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_X = X_test.values\n",
    "val_y = y_test.values\n",
    "\n",
    "\n",
    "\n",
    "#minmax = MinMaxScaler(feature_range=(0,1))\n",
    "\n",
    "val_X = scaler.fit_transform(val_X)\n",
    "val_y = minmax.fit_transform(val_y)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_X = val_X.reshape((val_X.shape[0], 1, val_X.shape[1]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a prediction\n",
    "y_pred = model.predict(val_X)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyplot.plot(pd.DataFrame(100*y_pred[1000:1200], index=  X_val.index[1000 : 1200]), label='Predction')\n",
    "pyplot.plot(pd.DataFrame(100*val_y[1000:1200], index=  X_val.index[1000 : 1200]), label='Actual')\n",
    "pyplot.legend()\n",
    "pyplot.xlabel('time stamps')\n",
    "pyplot.ylabel('% utilization')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyplot.plot(100*y_pred[4000:4200], label='Predction')\n",
    "pyplot.plot(100*val_y[4000:4200], label='Actual')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "100*mean_absolute_error(y_pred, val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.offline as py\n",
    "import plotly.graph_objs as go\n",
    "from plotly import __version__\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "\n",
    "traces = []\n",
    "\n",
    "#for turbine in random_turbines:\n",
    "trace = go.Scatter(x = X_val.index,\n",
    "                   y = y_pred,\n",
    "                   mode = 'lines',\n",
    "                   name = 'Prediction')\n",
    "traces.append(trace)\n",
    "\n",
    "py.iplot(traces, filename='line-mode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:automl]",
   "language": "python",
   "name": "conda-env-automl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
